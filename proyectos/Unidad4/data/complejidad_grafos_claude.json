[
  {
    "instruction": "Analiza la complejidad temporal y espacial de esta implementación de QuickSort.",
    "input": "def quicksort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quicksort(left) + middle + quicksort(right)",
    "output": "Claro viejito, Esta implementación de QuickSort tiene las siguientes características de complejidad:\n\nComplejidad Temporal:\n- Mejor caso: O(n log n) cuando el pivote divide el arreglo en partes aproximadamente iguales.\n- Caso promedio: O(n log n) con una buena selección de pivote.\n- Peor caso: O(n²) cuando el pivote siempre es el elemento más pequeño o más grande.\n\nComplejidad Espacial: O(n) debido a la creación de tres nuevas listas (left, middle, right) en cada llamada recursiva. Además, la profundidad de la recursión añade O(log n) en el mejor caso y O(n) en el peor caso.\n\nEsta implementación sacrifica eficiencia espacial por legibilidad, ya que crea copias de los datos en lugar de hacer ordenamiento in-place."
  },
  {
    "instruction": "Compara la eficiencia de MergeSort y QuickSort. ¿Cuándo usarías cada uno?",
    "input": "def merge_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    mid = len(arr) // 2\n    left = merge_sort(arr[:mid])\n    right = merge_sort(arr[mid:])\n    return merge(left, right)\n\ndef merge(left, right):\n    result = []\n    i = j = 0\n    while i < len(left) and j < len(right):\n        if left[i] <= right[j]:\n            result.append(left[i])\n            i += 1\n        else:\n            result.append(right[j])\n            j += 1\n    result.extend(left[i:])\n    result.extend(right[j:])\n    return result",
    "output": "Claro viejito, Comparación de complejidades:\n\nMergeSort:\n- Temporal: O(n log n) en todos los casos (mejor, promedio y peor).\n- Espacial: O(n) debido a los arreglos temporales durante el merge.\n- Ventaja: Estable y predecible, garantiza O(n log n).\n\nQuickSort:\n- Temporal: O(n log n) promedio, O(n²) peor caso.\n- Espacial: O(log n) con implementación in-place.\n- Ventaja: Más rápido en la práctica por mejor localidad de caché.\n\n¿Cuándo usar cada uno?\n- MergeSort: Cuando necesitas estabilidad, trabajas con datos enlazados, o requieres garantía de O(n log n).\n- QuickSort: Para arreglos en memoria con acceso aleatorio rápido, cuando el espacio es limitado y puedes tolerar O(n²) en casos raros."
  },
  {
    "instruction": "¿Es correcta esta implementación de búsqueda binaria? Analiza su complejidad.",
    "input": "def binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1",
    "output": "Claro viejito, La implementación es correcta y representa la versión iterativa estándar de búsqueda binaria.\n\nAnálisis de complejidad:\n\nComplejidad Temporal: O(log n)\n- En cada iteración, el espacio de búsqueda se reduce a la mitad.\n- Si tenemos n elementos, necesitamos como máximo log₂(n) comparaciones.\n- Ejemplo: para 1024 elementos, máximo 10 iteraciones (2¹⁰ = 1024).\n\nComplejidad Espacial: O(1)\n- Solo usa variables auxiliares (left, right, mid).\n- No hay llamadas recursivas ni estructuras adicionales.\n\nPrecondición importante: El arreglo debe estar ordenado. Si no lo está, la búsqueda binaria no garantiza encontrar el elemento incluso si existe."
  },
  {
    "instruction": "Identifica el problema de complejidad en este código y sugiere una mejora.",
    "input": "def find_duplicates(arr):\n    duplicates = []\n    for i in range(len(arr)):\n        for j in range(i + 1, len(arr)):\n            if arr[i] == arr[j] and arr[i] not in duplicates:\n                duplicates.append(arr[i])\n    return duplicates",
    "output": "Claro viejito, Problema identificado: Complejidad cuadrática con búsqueda lineal adicional.\n\nComplejidad actual:\n- Temporal: O(n²) debido al bucle anidado, más O(n) por la operación 'in' en cada iteración, resultando en O(n³) en el peor caso.\n- Espacial: O(n) para almacenar duplicados.\n\nMejora propuesta usando un conjunto:\n\ndef find_duplicates(arr):\n    seen = set()\n    duplicates = set()\n    for num in arr:\n        if num in seen:\n            duplicates.add(num)\n        else:\n            seen.add(num)\n    return list(duplicates)\n\nComplejidad mejorada:\n- Temporal: O(n) - un solo recorrido con operaciones de conjunto O(1).\n- Espacial: O(n) para los dos conjuntos.\n\nLa mejora reduce drásticamente el tiempo de O(n³) a O(n), sacrificando mínimamente espacio adicional."
  },
  {
    "instruction": "Analiza la complejidad de este algoritmo BFS para grafos.",
    "input": "from collections import deque\n\ndef bfs(graph, start):\n    visited = set()\n    queue = deque([start])\n    visited.add(start)\n    result = []\n    \n    while queue:\n        node = queue.popleft()\n        result.append(node)\n        \n        for neighbor in graph[node]:\n            if neighbor not in visited:\n                visited.add(neighbor)\n                queue.append(neighbor)\n    \n    return result",
    "output": "Claro viejito, Esta es una implementación correcta de BFS (Breadth-First Search).\n\nAnálisis de complejidad:\n\nComplejidad Temporal: O(V + E)\n- V = número de vértices (nodos)\n- E = número de aristas (edges)\n- Cada nodo se visita exactamente una vez: O(V)\n- Cada arista se examina exactamente una vez: O(E)\n- Las operaciones con set (add, in) son O(1) promedio\n\nComplejidad Espacial: O(V)\n- visited: almacena hasta V nodos\n- queue: en el peor caso puede contener O(V) nodos\n- result: almacena V nodos\n\nCaracterísticas importantes:\n- BFS garantiza encontrar el camino más corto en grafos no ponderados.\n- La cola (deque) asegura procesamiento FIFO.\n- El conjunto 'visited' previene ciclos infinitos."
  },
  {
    "instruction": "Compara BFS y DFS. ¿Cuál usarías para encontrar el camino más corto?",
    "input": "def dfs(graph, start, visited=None):\n    if visited is None:\n        visited = set()\n    visited.add(start)\n    result = [start]\n    \n    for neighbor in graph[start]:\n        if neighbor not in visited:\n            result.extend(dfs(graph, neighbor, visited))\n    \n    return result",
    "output": "Claro viejito, Análisis de DFS (Depth-First Search):\n\nComplejidad Temporal: O(V + E)\n- Similar a BFS, visita cada vértice y arista una vez.\n\nComplejidad Espacial: O(V)\n- visited: O(V)\n- Pila de recursión: O(V) en el peor caso (grafo lineal)\n\nComparación BFS vs DFS:\n\nBFS:\n- Usa cola (FIFO)\n- Explora por niveles\n- Garantiza camino más corto en grafos no ponderados\n- Mejor para encontrar distancias mínimas\n\nDFS:\n- Usa pila (recursión o explícita)\n- Explora en profundidad primero\n- NO garantiza camino más corto\n- Mejor para: detección de ciclos, ordenamiento topológico, componentes conectados\n\nRespuesta: Para encontrar el camino más corto en un grafo no ponderado, SIEMPRE usa BFS. BFS explora todos los nodos a distancia k antes de explorar nodos a distancia k+1, garantizando el camino mínimo."
  },
  {
    "instruction": "¿Cuál es la complejidad de este algoritmo? Explica el razonamiento.",
    "input": "def mystery_function(n):\n    result = 0\n    i = 1\n    while i < n:\n        result += i\n        i *= 2\n    return result",
    "output": "Claro viejito, Análisis de complejidad:\n\nComplejidad Temporal: O(log n)\n\nRazonamiento:\n- La variable i comienza en 1 y se duplica en cada iteración (i *= 2).\n- Secuencia de valores de i: 1, 2, 4, 8, 16, ..., hasta que i >= n\n- Esto es una progresión geométrica con razón 2.\n- Número de iteraciones: k, donde 2^k ≈ n, por tanto k = log₂(n)\n\nEjemplo:\n- n = 100: iteraciones ≈ 7 (2^7 = 128)\n- n = 1000: iteraciones ≈ 10 (2^10 = 1024)\n\nComplejidad Espacial: O(1)\n- Solo usa variables auxiliares constantes (result, i).\n\nEste patrón de duplicación (i *= 2) siempre indica complejidad logarítmica, similar a búsqueda binaria donde el espacio de búsqueda se divide (o multiplica) por 2."
  },
  {
    "instruction": "Optimiza este código para verificar si un número es primo.",
    "input": "def is_prime(n):\n    if n < 2:\n        return False\n    for i in range(2, n):\n        if n % i == 0:\n            return False\n    return True",
    "output": "Claro viejito, Análisis del código actual:\n\nComplejidad Temporal: O(n)\n- Verifica divisibilidad desde 2 hasta n-1, ejecutando n-2 iteraciones.\n\nOptimización propuesta:\n\nimport math\n\ndef is_prime_optimized(n):\n    if n < 2:\n        return False\n    if n == 2:\n        return True\n    if n % 2 == 0:\n        return False\n    for i in range(3, int(math.sqrt(n)) + 1, 2):\n        if n % i == 0:\n            return False\n    return True\n\nComplejidad Temporal optimizada: O(√n)\n\nMejoras aplicadas:\n1. Solo verifica hasta √n porque si n tiene un divisor mayor que √n, también tiene uno menor que √n.\n2. Verifica solo números impares (incremento de 2) después de descartar pares.\n3. Caso especial para n=2.\n\nEjemplo de mejora:\n- Para n=1000000, versión original: 999998 iteraciones\n- Versión optimizada: ~500 iteraciones\n- Reducción de O(n) a O(√n) es significativa para números grandes."
  },
  {
    "instruction": "¿Por qué este código tiene complejidad O(2^n)? Explica y propón una solución con programación dinámica.",
    "input": "def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)",
    "output": "Claro viejito, Explicación de O(2^n):\n\nEsta implementación recursiva naive recalcula los mismos valores múltiples veces. Para fib(5):\n- fib(5) llama a fib(4) y fib(3)\n- fib(4) llama a fib(3) y fib(2)\n- fib(3) se calcula 2 veces\n- fib(2) se calcula 3 veces\n- El árbol de recursión crece exponencialmente\n\nComplejidad actual:\n- Temporal: O(2^n) - cada llamada genera aproximadamente 2 llamadas más\n- Espacial: O(n) - profundidad máxima de la pila de recursión\n\nSolución con Programación Dinámica (Memoización):\n\ndef fibonacci_dp(n, memo={}):\n    if n <= 1:\n        return n\n    if n not in memo:\n        memo[n] = fibonacci_dp(n-1, memo) + fibonacci_dp(n-2, memo)\n    return memo[n]\n\nComplejidad optimizada:\n- Temporal: O(n) - cada valor se calcula solo una vez\n- Espacial: O(n) - almacena n valores en memo + O(n) pila\n\nReducción dramática: de O(2^n) a O(n). Para n=40, la diferencia es de miles de millones de operaciones a solo 40."
  },
  {
    "instruction": "Analiza la complejidad de esta búsqueda en una matriz ordenada.",
    "input": "def search_matrix(matrix, target):\n    if not matrix or not matrix[0]:\n        return False\n    \n    rows, cols = len(matrix), len(matrix[0])\n    row, col = 0, cols - 1\n    \n    while row < rows and col >= 0:\n        if matrix[row][col] == target:\n            return True\n        elif matrix[row][col] > target:\n            col -= 1\n        else:\n            row += 1\n    \n    return False",
    "output": "Claro viejito, Este es un algoritmo elegante para buscar en una matriz ordenada por filas y columnas.\n\nEstrategia: Comienza en la esquina superior derecha.\n- Si el valor actual > target: se mueve a la izquierda (col -= 1)\n- Si el valor actual < target: se mueve hacia abajo (row += 1)\n- Esto elimina una fila o columna completa en cada paso\n\nComplejidad Temporal: O(m + n)\n- m = número de filas\n- n = número de columnas\n- En el peor caso, recorre desde (0, n-1) hasta (m-1, 0)\n- Máximo m + n - 1 pasos\n\nComplejidad Espacial: O(1)\n- Solo usa variables auxiliares (row, col)\n\nAlternativa: Aplicar búsqueda binaria dos veces sería O(log m + log n), pero este enfoque es más simple de implementar. Si la matriz está completamente ordenada (como un arreglo 1D), podríamos usar búsqueda binaria pura en O(log(m*n))."
  },
  {
    "instruction": "Compara la complejidad de Dijkstra con lista de prioridad vs implementación ingenua.",
    "input": "Grafo con V nodos, E aristas ponderadas no negativas",
    "output": "Claro viejito, con lista de prioridad (heap) Dijkstra es O((V+E) log V); con implementación ingenua buscando el mínimo en O(V) cada vez, termina en O(V^2 + E). Para grafos densos V^2 y para esparsos el heap es claramente superior."
  },
  {
    "instruction": "¿Cuándo usar Bellman-Ford en vez de Dijkstra?",
    "input": "Red con pesos negativos por promociones de costo",
    "output": "Claro viejito, Bellman-Ford maneja pesos negativos y detecta ciclos negativos en O(V·E). Dijkstra falla con pesos negativos. Si tus pesos son no negativos, Dijkstra es más rápido; si hay negativos o necesitas saber si existe ciclo negativo, usa Bellman-Ford."
  }
]